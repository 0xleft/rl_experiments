{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bdddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46178817",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adoma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:55: UserWarning: It seems that your observation  is an image but its `dtype` is (int8) whereas it has to be `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
      "  warnings.warn(\n",
      "c:\\Users\\adoma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:63: UserWarning: It seems that your observation space  is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  warnings.warn(\n",
      "c:\\Users\\adoma\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\env_checker.py:76: UserWarning: The minimal resolution for an image is 36x36 for the default `CnnPolicy`. You might need to use a custom features extractor cf. https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "\n",
    "env = SoloPlayerEnv()\n",
    "check_env(env, warn=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3b2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "# Train the agent\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d64d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "env = make_vec_env(\n",
    "    SoloPlayerEnv,\n",
    "    n_envs=1,\n",
    "    rng=rng,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # for computing rollouts\n",
    ")\n",
    "\n",
    "def train_expert():\n",
    "    expert = PPO(\n",
    "        policy=MlpPolicy,\n",
    "        env=env,\n",
    "        seed=0,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.0,\n",
    "        learning_rate=0.0003,\n",
    "        n_epochs=10,\n",
    "        n_steps=64,\n",
    "    )\n",
    "    expert.learn(1_000)  # Note: change this to 100_000 to train a decent expert.\n",
    "    return expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions():\n",
    "    # expert = train_expert()  # uncomment to train your own expert\n",
    "    expert = download_expert()\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "        rng=rng,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde1b9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'rewards' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m             obs \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     model\u001b[38;5;241m.\u001b[39mlearn(\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     26\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mrun_eval\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     13\u001b[0m vec_env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[1;32m---> 15\u001b[0m rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode finished after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timesteps, total rewards: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewards\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'rewards' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "obs = vec_env.reset()\n",
    "\n",
    "def run_eval():\n",
    "    rewards = 0\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_env.step(action)\n",
    "        vec_env.render()\n",
    "        \n",
    "        rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode finished after {i+1} timesteps, total rewards: {rewards}\")\n",
    "            rewards = 0\n",
    "            obs = vec_env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    run_eval()\n",
    "    model.learn(1000)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
