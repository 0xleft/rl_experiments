{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bdddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46178817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "\n",
    "env = SoloPlayerEnv()\n",
    "check_env(env, warn=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "# Train the agent\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d64d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import games.tileman.envs.solo_player_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "env = make_vec_env(\n",
    "    \"tileman-solo-v0\",\n",
    "    n_envs=1,\n",
    "    rng=rng,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # for computing rollouts\n",
    ")\n",
    "\n",
    "def train_expert():\n",
    "    expert = PPO(\n",
    "        policy=MlpPolicy,\n",
    "        env=env,\n",
    "        seed=0,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.0,\n",
    "        learning_rate=0.0003,\n",
    "        n_epochs=10,\n",
    "        n_steps=64,\n",
    "    )\n",
    "    expert.learn(100_000)  # Note: change this to 100_000 to train a decent expert.\n",
    "    return expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions():\n",
    "    expert = train_expert()\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "        rng=rng,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "transitions = sample_expert_transitions()\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_env = make_vec_env(\n",
    "    \"tileman-solo-v0\",\n",
    "    rng=rng,\n",
    "    env_make_kwargs={\"render_mode\": \"human\"},  # for rendering\n",
    ")\n",
    "\n",
    "print(\"Evaluating the untrained policy.\")\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    evaluation_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward before training: {reward}\")\n",
    "\n",
    "print(\"Training a policy using Behavior Cloning\")\n",
    "bc_trainer.train(n_epochs=10)\n",
    "\n",
    "print(\"Evaluating the trained policy.\")\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    evaluation_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward after training: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cde1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Using cpu device\n",
      "9\n",
      "Episode finished after 1 timesteps, total rewards: [-0.1]\n",
      "6\n",
      "Episode finished after 1 timesteps, total rewards: [-0.1]\n",
      "9\n",
      "Episode finished after 1 timesteps, total rewards: [-0.1]\n",
      "9\n",
      "Episode finished after 1 timesteps, total rewards: [-0.1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m             obs \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m, in \u001b[0;36mrun_eval\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     22\u001b[0m obs, reward, done, info \u001b[38;5;241m=\u001b[39m vec_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mvec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\adomas\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:104\u001b[0m, in \u001b[0;36mDummyVecEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    Gym environment rendering. If there are multiple environments then\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    they are tiled together in one image via ``BaseVecEnv.render()``.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    :param mode: The rendering type.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adomas\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:256\u001b[0m, in \u001b[0;36mVecEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# call the render method of the environments\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# Create a big image by tiling images from subprocesses\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     bigimg \u001b[38;5;241m=\u001b[39m tile_images(images)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adomas\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:95\u001b[0m, in \u001b[0;36mDummyVecEnv.get_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     91\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe render mode is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but this method assumes it is `rgb_array` to obtain images.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs]\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [env\u001b[38;5;241m.\u001b[39mrender() \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs]\n",
      "File \u001b[1;32mc:\\Users\\adomas\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:95\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     91\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe render mode is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but this method assumes it is `rgb_array` to obtain images.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs]\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m env \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs]\n",
      "File \u001b[1;32mc:\\Users\\adomas\\anaconda3\\lib\\site-packages\\gymnasium\\core.py:471\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adomas\\Desktop\\projects\\rl_experiments\\games\\tileman\\envs\\solo_player_env.py:95\u001b[0m, in \u001b[0;36mSoloPlayerEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindow Name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_frame())\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_frame()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "# Train the agent\n",
    "\n",
    "vec_env = make_vec_env(SoloPlayerEnv, n_envs=1, env_kwargs=dict(grid_size=40, vision_range=5, max_steps=300))\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "#model.learn(total_timesteps=1_00, progress_bar=True)\n",
    "obs = vec_env.reset()\n",
    "\n",
    "def run_eval():\n",
    "    rewards = 0\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_env.step(action)\n",
    "        vec_env.render()\n",
    "        \n",
    "        rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode finished after {1} timesteps, total rewards: {rewards}\")\n",
    "            rewards = 0\n",
    "            obs = vec_env.reset()\n",
    "\n",
    "while True:\n",
    "    run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(\n",
    "    model.policy,  # type: ignore[arg-type]\n",
    "    vec_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Rewar: {reward}\")\n",
    "\n",
    "vec_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
