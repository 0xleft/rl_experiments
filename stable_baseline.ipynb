{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bdddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46178817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "\n",
    "env = SoloPlayerEnv()\n",
    "check_env(env, warn=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "# Train the agent\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d64d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import games.tileman.envs.solo_player_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "env = make_vec_env(\n",
    "    \"tileman-solo-v0\",\n",
    "    n_envs=1,\n",
    "    rng=rng,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # for computing rollouts\n",
    ")\n",
    "\n",
    "def train_expert():\n",
    "    expert = PPO(\n",
    "        policy=MlpPolicy,\n",
    "        env=env,\n",
    "        seed=0,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.0,\n",
    "        learning_rate=0.0003,\n",
    "        n_epochs=10,\n",
    "        n_steps=64,\n",
    "    )\n",
    "    expert.learn(100_000)  # Note: change this to 100_000 to train a decent expert.\n",
    "    return expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions():\n",
    "    expert = train_expert()\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "        rng=rng,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "transitions = sample_expert_transitions()\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_env = make_vec_env(\n",
    "    \"tileman-solo-v0\",\n",
    "    rng=rng,\n",
    "    env_make_kwargs={\"render_mode\": \"human\"},  # for rendering\n",
    ")\n",
    "\n",
    "print(\"Evaluating the untrained policy.\")\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    evaluation_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward before training: {reward}\")\n",
    "\n",
    "print(\"Training a policy using Behavior Cloning\")\n",
    "bc_trainer.train(n_epochs=10)\n",
    "\n",
    "print(\"Evaluating the trained policy.\")\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    evaluation_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward after training: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Using cpu device\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e02f1e1e3b41c49a222aca590b9746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    fps             | 36       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 35          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010790299 |\n",
      "|    clip_fraction        | 0.0561      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.00076     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 34          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006825514 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00924    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00852    |\n",
      "|    value_loss           | 7.62e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 34          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009533215 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0446      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00855    |\n",
      "|    value_loss           | 2.29e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 34          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006582068 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | nan         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 1.68e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pygame\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "# Train the agent\n",
    "\n",
    "vec_env = make_vec_env(SoloPlayerEnv, n_envs=1, env_kwargs=dict(grid_size=40, vision_range=5, max_steps=300))\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model.learn(total_timesteps=10_00, progress_bar=True)\n",
    "obs = vec_env.reset()\n",
    "\n",
    "def run_eval():\n",
    "    rewards = 0\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_env.step(action)\n",
    "        vec_env.render()\n",
    "        \n",
    "        rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode finished after {1} timesteps, total rewards: {rewards}\")\n",
    "            rewards = 0\n",
    "            obs = vec_env.reset()\n",
    "\n",
    "#while True:\n",
    "#    run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcc6a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewar: 0.0\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(\n",
    "    model.policy,  # type: ignore[arg-type]\n",
    "    vec_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Rewar: {reward}\")\n",
    "\n",
    "vec_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
