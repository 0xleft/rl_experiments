{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bdddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46178817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "\n",
    "env = SoloPlayerEnv()\n",
    "check_env(env, warn=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "# Train the agent\n",
    "model = A2C(\"MlpPolicy\", vec_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d64d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import games.tileman.envs.solo_player_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.util.util import make_vec_env\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "env = make_vec_env(\n",
    "    \"tileman-solo-v0\",\n",
    "    n_envs=1,\n",
    "    rng=rng,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # for computing rollouts\n",
    ")\n",
    "\n",
    "def train_expert():\n",
    "    expert = PPO(\n",
    "        policy=MlpPolicy,\n",
    "        env=env,\n",
    "        seed=0,\n",
    "        batch_size=64,\n",
    "        ent_coef=0.0,\n",
    "        learning_rate=0.0003,\n",
    "        n_epochs=10,\n",
    "        n_steps=64,\n",
    "    )\n",
    "    expert.learn(100_000)  # Note: change this to 100_000 to train a decent expert.\n",
    "    return expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions():\n",
    "    expert = train_expert()\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "        rng=rng,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "transitions = sample_expert_transitions()\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c5395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_env = make_vec_env(\n",
    "    \"tileman-solo-v0\",\n",
    "    rng=rng,\n",
    "    env_make_kwargs={\"render_mode\": \"human\"},  # for rendering\n",
    ")\n",
    "\n",
    "print(\"Evaluating the untrained policy.\")\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    evaluation_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward before training: {reward}\")\n",
    "\n",
    "print(\"Training a policy using Behavior Cloning\")\n",
    "bc_trainer.train(n_epochs=10)\n",
    "\n",
    "print(\"Evaluating the trained policy.\")\n",
    "reward, _ = evaluate_policy(\n",
    "    bc_trainer.policy,  # type: ignore[arg-type]\n",
    "    evaluation_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Reward after training: {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9796d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc3e3f2edd44b50b4b4016cdef40016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pygame\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from games.tileman.envs.solo_player_env import SoloPlayerEnv\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "vec_env = make_vec_env(\"tileman-solo-v0\", n_envs=4, env_kwargs=dict(grid_size=40, vision_range=10))\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations):\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=256),\n",
    "    normalize_images=False\n",
    ")\n",
    "\n",
    "model = PPO(\"CnnPolicy\", vec_env, verbose=0, n_steps=5000, batch_size=100, n_epochs=8, learning_rate=0.0002, policy_kwargs=policy_kwargs)\n",
    "# model = PPO.load(\"best_models\\\\1310000\\\\275.6666666666667\", print_system_info=True, env=vec_env)\n",
    "\n",
    "class SaveEvalCallback(BaseCallback):\n",
    "    def __init__(self, eval_freq=10000, verbose=0):\n",
    "        super(SaveEvalCallback, self).__init__(verbose)\n",
    "        self.eval_env = make_vec_env(\"tileman-solo-v0\", n_envs=1, env_kwargs=dict(grid_size=40, vision_range=10))\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -float('inf')\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            reward, _ = evaluate_policy(\n",
    "                model.policy,  # type: ignore[arg-type]\n",
    "                vec_env,\n",
    "                n_eval_episodes=3,\n",
    "                render=False,  # comment out to speed up\n",
    "            )\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Step: {self.n_calls}, Reward: {reward}\")\n",
    "            if reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = reward\n",
    "                self.model.save(f\"best_models/{self.n_calls}/{reward}\")\n",
    "        return True\n",
    "    \n",
    "model.learn(total_timesteps=100_000_000, progress_bar=True, callback=SaveEvalCallback(eval_freq=5000, verbose=1))\n",
    "obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7da61-dd56-4807-99b0-f126b9cd307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976dd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval():\n",
    "    rewards = 0\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = vec_env.step(action)\n",
    "        vec_env.render()\n",
    "        \n",
    "        rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode finished after {1} timesteps, total rewards: {rewards}\")\n",
    "            rewards = 0\n",
    "            obs = vec_env.reset()\n",
    "\n",
    "while True:\n",
    "    run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "reward, _ = evaluate_policy(\n",
    "    model.policy,  # type: ignore[arg-type]\n",
    "    vec_env,\n",
    "    n_eval_episodes=3,\n",
    "    render=False,  # comment out to speed up\n",
    ")\n",
    "print(f\"Rewar: {reward}\")\n",
    "\n",
    "vec_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
